# 32GB vGPU 配置 - 7B 4-bit SFT
# 适用：32GB 以上显存、支持 FlashAttention & BF16 的 Ampere+/Ada GPU
# 模型：DeepSeek R1 Distill Qwen 7B
# 量化：4-bit NF4 with LoRA

# ============ 模型配置 ============
model_name_or_path: ../Qwen2.5-7B-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2  # 需要安装 flash-attn>=2.6

# 4-bit 量化配置
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4

# LoRA 配置
use_peft: true
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05

# ============ 数据配置 ============
dataset_name: ../OpenR1-Math-220k-tokenized-Qwen-2.5-7B
dataset_config: default
dataset_preprocessing_num_proc: 4
eos_token: '<|im_end|>'

# ============ 训练配置 ============
# 32GB 显存推荐设置
bf16: true
fp16: false
per_device_train_batch_size: 3
per_device_eval_batch_size: 2
gradient_accumulation_steps: 6  # 全局 batch size ≈ 18 (单卡)
group_by_length: true
max_grad_norm: 0.5
learning_rate: 2.0e-04
num_train_epochs: 1
max_steps: -1

# 长序列设置
max_length: 4096
packing: false

# 优化器
optim: adamw_8bit
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
warmup_ratio: 0.03

# 梯度检查点和内存优化
gradient_checkpointing: false
use_liger_kernel: true

# ============ 日志和保存 ============
output_dir: ./output_7b_4bit_sft
overwrite_output_dir: false
do_eval: false
eval_strategy: 'no'

# 日志
log_level: info
logging_steps: 10
logging_strategy: steps
report_to:
  - wandb

# 保存策略
save_strategy: steps
save_steps: 500
save_total_limit: 2

# Hub 推送
push_to_hub: false
hub_model_id: open-r1/Qwen2.5-7B-Instruct-sft
hub_strategy: every_save
hub_private_repo: false
hub_always_push: true

# 其他配置
seed: 42
dataloader_drop_last: true
remove_unused_columns: true
